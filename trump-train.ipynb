{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useless Trump Tweet Generator\n",
    "This is a trump tweet generator based on Jeremy Howard's great MOOC at fast.ai.  Go watch it now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla K80 (CNMeM is enabled with initial size: 95.0% of memory, cuDNN 5105)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "from __future__ import division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import TimeDistributed, Activation\n",
    "from numpy.random import choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We haven't really looked into the detail of how this works yet - so this is provided for self-study for those who are interested. We'll look at it closely next week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 3306584\n"
     ]
    }
   ],
   "source": [
    "path = get_file('trump.csv', origin=\"https://github.com/bpb27/trump-tweet-archive/raw/master/data/realdonaldtrump/realdonaldtrump.csv\")\n",
    "import pandas as pd\n",
    "text_pd = pd.read_csv (path).as_matrix()[:,2]\n",
    "text_pd = [s.replace(\"&amp;\", \"&\") for s in text_pd]\n",
    "text = \"\\n\".join(text_pd)\n",
    "print('corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I will be interviewed from Cleveland, Ohio, on @seanhannity - Tonight at 10:00 P.M. Enjoy!', '\"@ThAllenSBoucher: @DiamondandSilk @realDonaldTrump @seanhannity I love those beautiful gals.\" D + S = Two amazing women!', '.@YoungDems4Trump  Thank you!', 'Great new polls! Thank you Nevada, North Carolina & Ohio. Join the MOVEMENT today & lets #MAGA!\\xe2\\x80\\xa6 https://t.co/Y8Sb8MNyXA', 'Thank you Toledo, Ohio! It is so important for you to get out and VOTE on November 8, 2016! Lets MAKE AMERICA SAFE\\xe2\\x80\\xa6 https://t.co/MQdp4GgLIE', 'RT @GMA: WATCH: @IvankaTrump on \"women who work;\" empowering campaign celebrates modern women.  https://t.co/rMFe9o6WcL', 'Hopefully the violence & unrest in Charlotte will come to an immediate end. To those injured, get well soon. We need unity & leadership.', 'The situations in Tulsa and Charlotte are tragic. We must come together to make America safe again.', 'It is a MOVEMENT - not a campaign. Leaving the past behind, changing our future. Together, we will MAKE AMERICA SAF\\xe2\\x80\\xa6 https://t.co/Lt2L3NKzyi', 'Thank you Kenansville, North Carolina! Remember- on November 8th, that special interest gravy train is coming to a\\xe2\\x80\\xa6 https://t.co/AysJRMzvKw', 'Thank you High Point, NC! I will fight for every neglected part of this nation & I will fight to bring us together\\xe2\\x80\\xa6 https://t.co/DSaUpSptBz', 'Hillary Clinton is taking the day off again, she needs the rest. Sleep well Hillary - see you at the debate!']\n"
     ]
    }
   ],
   "source": [
    "print (text_pd[1000:1012])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#path = 'data/wiki/'\n",
    "#text = open(path+'small.txt').read().lower()\n",
    "#print('corpus length:', len(text))\n",
    "\n",
    "#text = text[0:1000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 165\n"
     ]
    }
   ],
   "source": [
    "chars_ = sorted(list(set(text)))\n",
    "vocab_size = len(chars_)+1\n",
    "print('total chars:', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chars_.insert(0, \"\\0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000\n",
      "\r",
      " !\"#$%&'()*+,-./0123456789:;=?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]_`abcdefghijklmnopqrstuvwxyz{|}~����������������������������������������������������������������������\n"
     ]
    }
   ],
   "source": [
    "print (''.join(chars_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000\n",
      "\r",
      " !\"#$%&'()*+,-./0123456789:;=?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]_`abcdefghijklmnopqrstuvwxyz{|}~\n",
      "total chars: 95\n"
     ]
    }
   ],
   "source": [
    "char_indices = dict((c, i) for i, c in enumerate(chars_))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars_))\n",
    "\n",
    "tilde_idx = chars_.index('~')+1\n",
    "chars = chars_[:tilde_idx]\n",
    "print (''.join(chars))\n",
    "\n",
    "vocab_size = len(chars)\n",
    "print('total chars:', vocab_size)\n",
    "\n",
    "import pickle\n",
    "pickle.dump(char_indices, open(\"data/trump_char_indices.pkl\", \"w\"))\n",
    "pickle.dump(indices_char, open(\"data/trump_indices_char.pkl\", \"w\"))\n",
    "pickle.dump(chars, open(\"data/trump_chars.pkl\", \"w\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before clipping: vocab size is  164\n",
      "after clipping: vocab size is  94\n"
     ]
    }
   ],
   "source": [
    "idx = [char_indices[c] for c in text]\n",
    "print (\"before clipping: vocab size is \", max(idx))\n",
    "idx = np.array(idx).clip(0, tilde_idx-1)\n",
    "print (\"after clipping: vocab size is \", max(idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3306584\n"
     ]
    }
   ],
   "source": [
    "print (len(idx))\n",
    "idx[:10]\n",
    "idx = idx[:1000000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Jackie Evancho's album sales have skyrocketed after announcing her Ina\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(indices_char[i] for i in idx[:70])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess and create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 999961\n"
     ]
    }
   ],
   "source": [
    "maxlen = 40\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(idx) - maxlen+1):\n",
    "    sentences.append(idx[i: i + maxlen])\n",
    "    next_chars.append(idx[i+1: i+maxlen+1])\n",
    "print('nb sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = np.concatenate([[np.array(o)] for o in sentences[:-2]])\n",
    "next_chars = np.concatenate([[np.array(o)] for o in next_chars[:-2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((999959, 40), (999959, 40))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.shape, next_chars.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_fac = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/topology.py:368: UserWarning: The `regularizers` property of layers/models is deprecated. Regularization losses are now managed via the `losses` layer/model property.\n",
      "  warnings.warn('The `regularizers` property of '\n"
     ]
    }
   ],
   "source": [
    "model=Sequential([\n",
    "        Embedding(vocab_size, n_fac, input_length=maxlen),\n",
    "        LSTM(512, input_dim=n_fac,return_sequences=True, dropout_U=0.2, dropout_W=0.2,\n",
    "             consume_less='gpu'),\n",
    "        Dropout(0.2),\n",
    "        LSTM(512, return_sequences=True, dropout_U=0.2, dropout_W=0.2,\n",
    "             consume_less='gpu'),\n",
    "        Dropout(0.2),\n",
    "        TimeDistributed(Dense(vocab_size)),\n",
    "        Activation('softmax')\n",
    "    ])\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"data/trump_epoch0_0.000.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_example():\n",
    "    seed_string=\"ethics is a basic foundation of all that\"\n",
    "    for i in range(2000):\n",
    "        x=np.array([char_indices[c] for c in seed_string[-40:]])[np.newaxis,:]\n",
    "        preds = model.predict(x, verbose=0)[0][-1]\n",
    "        preds = preds/np.sum(preds)\n",
    "        next_char = choice(chars, p=preds)\n",
    "        seed_string = seed_string + next_char\n",
    "    return seed_string[40:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "999959/999959 [==============================] - 1858s - loss: 1.7404  \n",
      "Epoch 1/1\n",
      "999959/999959 [==============================] - 1857s - loss: 1.5587  \n",
      "Epoch 1/1\n",
      "999959/999959 [==============================] - 1857s - loss: 1.4956  \n",
      "Epoch 1/1\n",
      "999959/999959 [==============================] - 1857s - loss: 1.4566  \n"
     ]
    }
   ],
   "source": [
    "tweets = list()\n",
    "for lr in [1e-4, 1e-5]:\n",
    "    for epoch in [0, 1]:\n",
    "        model.optimizer.lr=lr\n",
    "        model.fit(sentences, np.expand_dims(next_chars,-1), batch_size=64, nb_epoch=1)\n",
    "        model.save_weights('data/trump_epoch%d_%3.6f.h5' % (epoch, lr))\n",
    "    tweets.append (print_example())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets = print_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " at never mention DonaldTrump has asking worse, great speech and vets at oor with Like Aberica great again.\" She can't be am including Nov over the people of Nc government.\"\n",
      "~~~@realDonaldTrump Rink who is about my great?\n",
      "Via @bredlerding Olama was going down and special interests made voters past. Good luck we got the reduce w/ OK what you're making my support Donald Trump\n",
      "Thank you for you! #MakeAmericaGreatAgain\"\n",
      "\"@GarkeOSson: @realDonaldTrump I think low lost!\n",
      "Thank you @tedcruz can't win in fistried to give $2$. Clinton. What is in order to say that I don't survived reason doing in Iowa. When they would love @marcorubio. Watching it!\n",
      "\"@AjboryGen: @megynkelly\n",
      "Hillary looks like social interests to the Saturday at 11:30. A win- he went his ording!\n",
      "I will be in Carallove, who are they never say Wondly and the borders are speaking a true mention that they would be forgotten, now days last night - 10%!\n",
      "Thank you New Hampshire! #trump2016 #MAGA https://t.co/3NfOAqLtSj\n",
      "Iowa made sure a statements to do why reparding the VE @oreillyfactor tonight at 7pm. Enjoy! #ImWithYou https://t.co/HwncWUdG4u\n",
      "Wow, South Carolina! Get out and VOTE! #Trump2016 \n",
      "#MakeAmericaGreatAgain https://t.co/kc1EQ24YhW via @garline: Problem!\n",
      "Will be talking about you! You are working American honor? Media wanting for having a great y bigger in NYC. He can~~~t uher that Chris Wallace Hillary Clinton sleazers as the WH park Bush!!!?! https://t.co/RkniLIHEum\n",
      "Just like 20% Trump poll numbers that for the party-Wow, @oreillyfactor and special protect his places that (http://t.co/Z7GrxueisH\n",
      ".@TMUTW @JoeNBC    http://t.co/xgyyhB8qus\n",
      "Thank you, be wonder with Check to Turnberry, President all of our countrying it and very but can't coling the Thuck Joe. Ifs for people givet this speech in Sunday to Benorial U.S. close the results of race. A true, BIG AND GREAT AGAINE doesn't have antweed of the admitting the party of us more where he going for changed how book anaiged leading the presidential businessman\n"
     ]
    }
   ],
   "source": [
    "print (tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save_weights(\"data/trump_final_data.h5\")\n",
    "\n",
    "j = model.to_json()\n",
    "f = open(\"data/trump_model.json\", \"w\")\n",
    "f.write(j)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, go look at trump-generate.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
